{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedRayeneRomdhane/tools-ai/blob/main/practices/L01%20-%20Linear%20Regression%20with%20One%20Variable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KXrNWH9HU1T"
      },
      "source": [
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/logo.png?raw=1\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyxM4p-nHU1V"
      },
      "source": [
        "Made by **Balázs Nagy** and **Márk Domokos**\n",
        "\n",
        "[<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/open_button.png?raw=1\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L01%20-%20Linear%20Regression%20with%20One%20Variable.ipynb)\n",
        "\n",
        "# Lab 01: Linear regression with one variable\n",
        "### Profit in a city\n",
        "In this exercise, we will implement a univariate linear regression model to predict the profit of a food chain company.\n",
        "\n",
        "Imagine you are the CEO of a food chain company and thinking about where to open a new store, in which cities it would be beneficial. The food chain company already has the conditions to do this, you just need to decide which cities to expand in. To do this the company has collected data on the population of the cities and the realized profits so far. The task is to use this data for prediction as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzRX-TQpHU1W"
      },
      "source": [
        "### 1: Import the packages\n",
        "We will need some elements of matplotlib for drawing and NumPy for easier array handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-2lddGHHU1X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uep0lXWZHU1Y"
      },
      "source": [
        "### 2: Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js4Es9IgHU1Z"
      },
      "source": [
        "The data will be loaded from a publicly available file. An alternative solution would be to upload the data file directly to the google colab file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9BQEWCfHU1Z"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab01/Lab1data.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-imgXrdHU1b"
      },
      "source": [
        "Sort your data into the appropriate variables. First we define 2 lists, X and Y. Moving down the line, we split the data along the separator character. Then we transform the data into a NumPy array and format it into 2 column vectors. At the end of the operation, the dimension of both arrays will be m x 1, where m is the number of samples. X will contain the input variables and Y will contain the so called labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfa-O18KHU1b"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Lab1data.txt',header = None).to_numpy()\n",
        "X = data[:,0:1]                                                # sort X\n",
        "m = X.shape[0]                                                 # number of samples\n",
        "Y = data[:,1].reshape(m,1)                                     # sort Y\n",
        "\n",
        "print('X:',X.shape)                                            # check the shapes\n",
        "print('Y:',Y.shape)\n",
        "print('Number of samples: ',m)\n",
        "#print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocxSJPfKHU1c"
      },
      "source": [
        "### 3: Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIM6HUDHU1c"
      },
      "source": [
        "Plot the data on a graph to see the structure of the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-uylav0HU1d"
      },
      "outputs": [],
      "source": [
        "plt.plot(X,Y,'o', c= \"g\")\n",
        "plt.title(\"Training data\")\n",
        "plt.xlabel(\"Population of a city in 10 000s\")\n",
        "plt.ylabel(\"Profit in $10 000s \")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhF-1VBhHU1d"
      },
      "source": [
        "So, for example, the graph shows that a city with a population of 150,000 can expect to make a profit of about $120,000. This is our labelled data, so each sample has its expected output. In our example, the population data will be the input and the expected profit will be the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rvjb6yFHU1e"
      },
      "source": [
        "### 4: Linear regression model\n",
        "\n",
        "We will try to approximate the data on the graph with a line, which will be our starting hypothesis.\n",
        "Our linear regression model can be written as follows:\n",
        "\n",
        "$ h_{w}(x) = w_{0} + xw_{1}  = \\hat{y}$\n",
        "\n",
        ",where <br>\n",
        "$h$ - hypothesis<br>\n",
        "$x$ - input data - so called feature. (Only one in this model)<br>\n",
        "$w$ - weights <br>\n",
        "$\\hat{y}$ - prediction (or estimated value)\n",
        "\n",
        "The block diagram of the algorithm is illustrated in the figure below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCNTgVZSHU1e"
      },
      "source": [
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/Lab01/Pics/L01_Model.png?raw=1\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN2gx2SFHU1e"
      },
      "source": [
        "The goal is to get the estimated profit output given the size of a city according to our hypothesis. To do this, we first need to set up our hypothesis. We specify the model, which in our case is a linear fit, and then we set the parameters using the available samples. It is called the training of the model.\n",
        "\n",
        "To test the fit of the model we need to define a cost function. The cost function will determine how good our model is in a given iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSJC-L7XHU1f"
      },
      "source": [
        "### 5. Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTfOLhtNHU1f"
      },
      "source": [
        "The cost function (also called Error function) in this approach is the MSE (Mean Squared Error) function, which can be written as follows:\n",
        "\n",
        "$C=\\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y}^{i}-y^{i})^2$\n",
        "\n",
        "This is the sum of the squared differences between our estimates and our labels for the samples. Thus, the error will always be a positive number, the smaller the better our estimate. The constant multiplier term corresponding to the sample number will play more of a role later on. The nature of the error term is not affected.\n",
        "\n",
        "Inserting our hypothesis we obtain the following form:\n",
        "\n",
        "$C=\\frac{1}{2m}\\sum_{i=1}^{m}(h_w(x^{i})-y^{i})^2$\n",
        "\n",
        "And in its final complete form:\n",
        "\n",
        "$ C(w_{0},w_{1})=\\frac{1}{2m} \\cdot \\sum_{i=1}^{m}(w_{0} + x^iw_{1} − y^{i})^2 $\n",
        "\n",
        "As we have defined our hypothesis in a way that we can adjust the offset of the line with the weight $w_0$ in addition to the slope of the line, it is useful to introduce a variable to facilitate matrix operations. Let $x_0=1$, so we have extended our single variable input with a BIAS member.\n",
        "\n",
        "$ C(w_{0},w_{1})=\\frac{1}{2m} \\cdot \\sum_{i=1}^{m}(w_{0}x^{i}_{0} + w_{1}x^{i}_{1} − y^{i})^2 $\n",
        "\n",
        "With this extension, where the constans 1 member is also considered as an input variable, the matrix specification is facilitated.\n",
        "\n",
        "In matrix form, we can write the cost function as follows:\n",
        "\n",
        "$ C = \\frac{1}{2m} \\cdot \\sum(XW-Y)^2 $\n",
        "\n",
        "Let's review the matrices used in this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE5cS_NGHU1f"
      },
      "source": [
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/Lab01/Pics/L01_Matrixok.png?raw=1\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGap3jFsHU1f"
      },
      "source": [
        "With the help of these matrices, our estimation per sample and the calculation of the cost function for all samples can be done easily.\n",
        "\n",
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/Lab01/Pics/L01_CostCalculation.png?raw=1\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mvKPvufHU1f"
      },
      "source": [
        "Create the necessary matrices! Initialize the weights with 0!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSsmsy9HHU1f"
      },
      "outputs": [],
      "source": [
        "################### CODE HERE ########################\n",
        "# Initialize the  W vector with the right size\n",
        "# Extend X with the BIAS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iF_1VwoHU1f"
      },
      "source": [
        "Check the dimension of your arrays before performing any matrix operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw7bRkqnHU1f"
      },
      "outputs": [],
      "source": [
        "print('X dimensions:\\n', X.shape)\n",
        "print('Y dimensions:\\n', Y.shape)\n",
        "print('W dimensions:\\n', W.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9gjn-f7HU1g"
      },
      "source": [
        "### Cost function:\n",
        "Define a function to compute the cost function that takes as input the matrices $X$, $Y$ and $W$ and returns the computed scalar cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1xuOP7fHU1g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def computeCost(X, Y, W):\n",
        "    m = len(Y)\n",
        "    Y_hat = X.dot(W)\n",
        "    errors = Y_hat - Y.reshape(-1,1)\n",
        "    C = (1/(2*m)) * np.sum(errors**2)\n",
        "    return C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXYh1sB4HU1g"
      },
      "outputs": [],
      "source": [
        "C1=computeCost(X,Y,np.array([[0],[0]]).reshape((2,1)))\n",
        "print('''Test (Cost function):\n",
        "\\tWeights: W = [0;0]\n",
        "\\tApproximate ground trueth value = 32.07\n",
        "\\tCalculated value = ''',C1)\n",
        "C2=computeCost(X,Y,np.array([[-1],[2]]))\n",
        "print('''\\n\\tWeights: W = [-1;2]\n",
        "\\tApproximate ground trueth value = 54.24\n",
        "\\tCalculated value = ''',C2)\n",
        "\n",
        "if int(C1) == 32 and int(C2) ==54:\n",
        "    print(\"\\n The computeCost function works properly.\")\n",
        "else:\n",
        "    print(\"\\n Something went wrong!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VUjl7DsHU1g"
      },
      "source": [
        "### 6. Gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgbv6uC3HU1g"
      },
      "source": [
        "After defining the cost function, the task is to minimize the cost function. We aim for an optimum. To find the minimum point we will use the gradient descent method.\n",
        "\n",
        "The gradient descent method consists of calculating the gradient (derivative) of the function at a given point. The derivative in our case effectively defines the tangent line of the function at that point. Accordingly, we can determine where we need to move on the function to get closer to the minimum. At the minimum point, the derivative of the function is 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruW6I5oRHU1g"
      },
      "source": [
        "<img src=\"https://github.com/Fortuz/edu_Adaptive/blob/main/practices/assets/Lab01/Pics/L01_Gradient.png?raw=1\" width=\"450\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuvbeRYxHU1g"
      },
      "source": [
        "We calculate the gradient (partial derivatives) of the cost function to modify the weights. The modifications are continued until the algorithm converges to the minimum point.\n",
        "\n",
        "The weight update of the gradient method can be described by the following formula:\n",
        "\n",
        "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w_0,w_1)$\n",
        "\n",
        ",where <br>\n",
        "$\\mu$ - learning rate, parameter to set the speed of convergence. $0 < \\mu < 1$.\n",
        "\n",
        "When $\\mu$ is chosen small, convergence slows down but approaches the minimum point more closely <br>\n",
        "When $\\mu$ is chosen high, it can cause divergence, which can lead to the algorithm crashing.\n",
        "\n",
        "One thing to keep in mind when using this method is that the weights should be adjusted simultaneously and in synchrony.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPJ_-P8yHU1g"
      },
      "source": [
        "### 7. Gradient descent applied to linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd_ILFChHU1g"
      },
      "source": [
        "Hypothesis and cost function of the linear regression model:\n",
        "\n",
        "$ h_w(x)=w_0+w_1x $\n",
        "\n",
        "$ \\color{blue}{C(w_0,w_1)}=\\frac{1}{2m}\\sum_{i=1}^{m}(h_w(x^{i})-y^{i})^2 $\n",
        "\n",
        "Weight update of the gradient descent method:\n",
        "\n",
        "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}\\color{blue}{C(w_0,w_1)}$\n",
        "\n",
        "Substituting the linear regression model into the gradient descent method for the partial derivative, we obtain the following:\n",
        "\n",
        "$ \\frac{\\partial}{\\partial w_j}C(w_0,w_1)=\\frac{\\partial}{\\partial w_j}\\frac{1}{2m}\\sum_{i=1}^{m}(h_w(x^{i})-y^{i})^2\n",
        "=\\frac{\\partial}{\\partial w_j}\\frac{1}{2m}\\sum_{i=1}^{m}(w_0+w_1x^i-y^{i})^2 $\n",
        "\n",
        "Performing the partial derivation according to the weights.\n",
        "\n",
        "$ \\color{red}{(j=0)}\\hspace{7mm} \\frac{\\partial}{\\partial w_j}C(w_0,w_1)=\\frac{1}{m}\\sum_{i=1}^{m}(w_0+w_1x^i-y^{i})\\cdot 1=\\frac{1}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i})\\cdot \\color{red}{x_0^i}) $\n",
        "\n",
        "$ \\color{red}{(j=1)}\\hspace{7mm} \\frac{\\partial}{\\partial w_j}C(w_0,w_1)=\\frac{1}{m}\\sum_{i=1}^{m}(w_0+w_1x^i-y^{i})\\cdot x_1^i=\\frac{1}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i})\\cdot x_1^i) $\n",
        "\n",
        "So accordingly, the weight updates in our example are given by:\n",
        "\n",
        "$ w_0=w_0-\\frac{\\mu}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i}) \\cdot \\color{red}{x_0^i}) $\n",
        "\n",
        "$ w_1=w_1-\\frac{\\mu}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i}) \\cdot x^i_1) $\n",
        "\n",
        "Since we introduced $ x_0 $ as a variable, the weight update formula generalizes well to different cases. Care should also be taken to update the weights simultaneously during the encoding, as asynchronous updating may cause a counting error!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DOQnKRdHU1h"
      },
      "source": [
        "We continue the process until we reach the optimum point or until we reach a desired number of iterations. So let's define a learning rate and an epoch number, which will limit the number of desired updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toi0Z8ADHU1h"
      },
      "outputs": [],
      "source": [
        "epochs = 1500\n",
        "learning_rate =0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-MsBJbSHU1h"
      },
      "source": [
        "Define a function that implements the gradient method. Within the function, store the previous values of the cost function in the variable $C_{history}$ for later evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzqExxHVHU1h"
      },
      "outputs": [],
      "source": [
        "def gradientDescent(X, Y, W, learning_rate, epochs):\n",
        "    m = Y.size\n",
        "    C_history = np.zeros((epochs,1))\n",
        "\n",
        "################### CODE HERE ########################\n",
        "# Implement the Gradient Descent weight update\n",
        "# Warning: simultaneous update!!!!!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################################################\n",
        "\n",
        "    return W,C_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwoUS7shHU1h"
      },
      "source": [
        "Let's run the function using the gradient descent method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhjFhuRoHU1h"
      },
      "outputs": [],
      "source": [
        "print('''\\n gradientDescent() function test (learning_rate=0.01):\n",
        "\\tWeights expected (approx.):\n",
        "\\t [-3.6303] [1.1664]''')\n",
        "W,C_history=gradientDescent(X,Y,W,learning_rate,epochs)\n",
        "print('\\tWeights calculated:\\n\\t',W[0],W[1])\n",
        "\n",
        "if (W[0]+3.6303) < 0.1 and (W[1]-1.1664) < 0.1:\n",
        "    print(\"\\n A gradientDescent function is good. You can proceed.\")\n",
        "else:\n",
        "    print(\"\\n Something went wrong!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyn-Ztl3HU10"
      },
      "source": [
        "### 8. Visualization of the results\n",
        "\n",
        "#### Display of cost function during iterations:\n",
        "\n",
        "Plot the values of the cost function over the iterations. This will show how our algorithm actually converges during the epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AsvQEh8HU10"
      },
      "outputs": [],
      "source": [
        "plt.plot(C_history)\n",
        "plt.title(\"Cost function of Gradient descent algorithm\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS50Pu4PHU10"
      },
      "source": [
        "### Display the fitted line on the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCGTJDOTHU10"
      },
      "outputs": [],
      "source": [
        "plt.plot((X[:,1]).reshape(97,1),Y,'o', label = \"Training data\")\n",
        "plt.plot((X[:,1]).reshape(97,1),X@W,'-',label = \"Linear regression\")\n",
        "plt.xlabel(\"Population of a city in 10 000s\")\n",
        "plt.ylabel(\"Profit in $10 000s \")\n",
        "plt.title(\"Linear regression and the training data\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyGPgIgiHU10"
      },
      "source": [
        "### 9. Prediction with the trained model\n",
        "To use the model, estimate the expected profit in a city of 10000 and 17000 inhabitants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iowqTVq9HU10"
      },
      "outputs": [],
      "source": [
        "Prediction1 = (np.array([1, 10]))@W\n",
        "Prediction2 = (np.array([1, 17]))@W\n",
        "print('\\nPrediction for 10 000 citizens:')\n",
        "print(Prediction1 * 10000)\n",
        "print('\\nPrediction for 17 000 citizens:')\n",
        "print(Prediction2 * 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f3A_o8MHU11"
      },
      "source": [
        "### Display cost function (surface plot)\n",
        "For a better illustration, it is worth examining the cost function over a larger interval. To do this, create a vector for each of the two weights and calculate the cost for each pair of values. This will give us an idea of the surface on which the optimum point was sought and where it was found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCd4Zq3iHU11"
      },
      "outputs": [],
      "source": [
        "w0_vals = np.linspace(-10,10,100)\n",
        "w1_vals = np.linspace(-1,4,100)\n",
        "C_vals = np.zeros((w0_vals.size,w1_vals.size))\n",
        "\n",
        "for i in range((w0_vals).size):\n",
        "    for j in range((w1_vals).size):\n",
        "        t=np.array([w0_vals[i],w1_vals[j]]).reshape(2,1)\n",
        "        C_vals[[i],[j]]= computeCost(X,Y,t)\n",
        "C_vals=C_vals.T\n",
        "\n",
        "fig= plt.figure()\n",
        "ax=plt.axes(projection='3d')\n",
        "x, y = np.meshgrid(w0_vals, w1_vals)\n",
        "surf = ax.plot_surface(x, y, C_vals, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
        "plt.title(\"Surface plot of the Cost function\")\n",
        "plt.xlabel(\"w0\")\n",
        "plt.ylabel(\"w1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQVboewiHU11"
      },
      "source": [
        "### Display cost function (contour plot)\n",
        "The contour plot is also a useful visualisation option, which has the advantage of flattening our 3D surface into 2D for easier transparency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhCd07T-HU11"
      },
      "outputs": [],
      "source": [
        "plt.contour(w0_vals,w1_vals,C_vals,np.logspace(-2,3,20))\n",
        "plt.plot(W[0],W[1],'x')\n",
        "plt.title(\"Contour plot of C_vals in logarithmic scale\")\n",
        "plt.xlabel(\"w_0\")\n",
        "plt.ylabel(\"w_1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmbij5zEHU11"
      },
      "source": [
        "## Once again. Using higher level packages\n",
        "Python and its advanced packages allow you to write much more compact code, making it easier to prototype quickly. Let's see how the example we looked at in the exercise can be more concisely solved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYNwPg4eHU11"
      },
      "outputs": [],
      "source": [
        "import pandas as pd                                  # Pandas for data handling\n",
        "from sklearn.linear_model import LinearRegression    # sklearn implementation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv('Lab1data.txt', header = None)\n",
        "XX = data.iloc[:, 0].values.reshape(-1, 1)           # Separate data\n",
        "YY = data.iloc[:, 1].values.reshape(-1, 1)           # Separate data\n",
        "\n",
        "\n",
        "lin_reg = LinearRegression()                         # Set linear regression class\n",
        "lin_reg.fit(XX,YY)                                   # Fit linear regression\n",
        "pred1 = lin_reg.predict([[10]])                      # Prediction for 10000\n",
        "pred2 = lin_reg.predict([[17]])                      # Prediction for 17000\n",
        "\n",
        "print('Prediction for a city with a population of 10 000: %.2f $' % (pred1*10000)[0,0])\n",
        "print('Prediction for a city with a population of 17 000: %.2f $' % (pred2*10000)[0,0])\n",
        "\n",
        "regline = lin_reg.predict([[5],[22.5]])              # Linear regression equatation\n",
        "\n",
        "plt.scatter(XX,YY, label = \"Training data\")          # Plot\n",
        "plt.plot([[5],[22.5]],regline,'r')\n",
        "plt.xlabel(\"Population of a city in 10 000s\")\n",
        "plt.ylabel(\"Profit in $10 000s \")\n",
        "plt.title(\"Linear regression and the training data\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cvhuN_xHU11"
      },
      "source": [
        "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}